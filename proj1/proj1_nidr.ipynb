{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df169d8-4c37-4450-97b5-6cc781a2ae4a",
   "metadata": {},
   "source": [
    "# DEEE725 Speech Signal Processing Lab\n",
    "### 2023 Spring, Kyungpook National University \n",
    "### Instructor: Gil-Jin Jang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09645527-0887-405f-9a9a-05da9cb45270",
   "metadata": {},
   "source": [
    "# Project 1 Isolated digit recognition in noisy environments\n",
    "\n",
    "- Assigned: 2023/04/21\n",
    "- Due: 2023/05/04\n",
    "- Required dataset: \n",
    "    1. [training data](lab05.pdf)\n",
    "    1. [validation data](lab05.md)\n",
    "    1. [test data](lab05.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8c951-5371-4e28-af69-2854658a82e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "# import packages, define analysis parameters and draw parameters, audio file preparation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9515d0d3-8af0-47b1-bba5-aedbf3a1f4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary pacakages\n",
    "# strange issue: keep the import order to prevent matplotlib error\n",
    "#  import matplotlib -> librosa -> pyplot -> librosa.display\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "#from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fft import fftshift\n",
    "\n",
    "# display wav files\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc26acb-6267-4773-8f15-ca4abb077ff4",
   "metadata": {},
   "source": [
    "오디오 파일들의 경로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827c344e-09c7-46fe-b841-0d3ad0f05dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add '/' if path is not a null string\n",
    "def addpath(path, file):\n",
    "    if len(path) == 0: \n",
    "        return file\n",
    "    else:\n",
    "        return path + '/' + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2fbc717-5dc6-43a9-b7c3-9f8f2b06506f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary pacakages\n",
    "# strange issue: keep the import order to prevent matplotlib error\n",
    "#  import matplotlib -> librosa -> pyplot -> librosa.display\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "#from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fft import fftshift\n",
    "\n",
    "# display wav files\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a593f2-a698-4eb8-b876-9d53cda83ebe",
   "metadata": {},
   "source": [
    "신호 분석과 스펙트럼을 그리기 위한 다음의 parameter 들을 정의한다.\n",
    "입력 파일의 sampling frequency 를 이용하여 shift size 를 sample 수로 정의하기 위해 사용된다.\n",
    "- `Ts`: shift length in seconds, default 0.01 sec = 10 ms. \n",
    "- `Tf`: frame length in seconds, default 0.02 sec = 20 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94df199-6bc7-4a5f-9c4e-50ee25d7aebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters for signal analysis\n",
    "# Fs = 16000  native sampling frequency (wav file 에 정의된 것) 을 사용하면 필요 없음\n",
    "Ts = 0.01   # 10 ms shift size\n",
    "Tf = 0.02   # 20 ms frame size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43cec4-4631-40db-9f72-1d2827a21db0",
   "metadata": {
    "tags": []
   },
   "source": [
    "spectrum 을 그리기 위한 parameters.\n",
    "- `cmap_plot`: colormap. default value is `pyplot.cm.bone_r` (최소값 흰색, 최대값 검은색 의 gray scale) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d45a30-c37a-4ddc-9a0a-8f3f3275e202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters for drawing\n",
    "#cmap_plot = plt.cm.bone # default colormap for spectrogram, gray\n",
    "cmap_plot = plt.cm.bone_r # default colormap for spectrogram, gray, reversed\n",
    "#cmap_plot = plt.cm.plasma \n",
    "#cmap_plot = plt.cm.inferno\n",
    "#FIG_SIZE = (15,10)   # obsolete\n",
    "FIG_SIZE = (8,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3a5e5-0083-47ae-9916-715c6466fb2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### 이전 lab 들에서 정의한 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0a2f81-d319-4137-99ee-2dde50c70ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw spectrogram\n",
    "from gjdrawspectrogram3 import drawspectrogram3\n",
    "\n",
    "# linear phase FIR filter design from magnitudes of the frequency components\n",
    "from gjfiroverlapadd import getLPHFIRFFT\n",
    "\n",
    "# trapezoidal overlap add for FIR filtering\n",
    "from gjfiroverlapadd import firoverlapadd\n",
    "\n",
    "# save audio in wav format\n",
    "import gjwavfile as wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19179671-401d-446e-ae50-3cbd61fab3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw spectrogram\n",
    "from gjdrawspectrogram3 import drawspectrogram3\n",
    "\n",
    "# linear phase FIR filter design from magnitudes of the frequency components\n",
    "from gjfiroverlapadd import getLPHFIRFFT\n",
    "\n",
    "# trapezoidal overlap add for FIR filtering\n",
    "from gjfiroverlapadd import firoverlapadd\n",
    "\n",
    "# save audio in wav format\n",
    "import gjwavfile as wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe1916-81e9-4942-bd0b-d105a74bf6d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### load speech and noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ce62b-6c74-4d57-9cf9-73ee1dca1ae7",
   "metadata": {},
   "source": [
    "오디오 파일이 16 kHz, mono 인지 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631adcd2-1699-4f7c-b75a-2c4a75d4c40d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1\n",
      "10 2\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros(10)\n",
    "print(len(x), x.ndim)\n",
    "x = np.zeros((10,2))\n",
    "print(len(x), x.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32a41e69-b1b9-4c9f-b6b5-bce72ee5d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add '/' if path is not a null string\n",
    "def addpath(path, file):\n",
    "    if len(path) == 0: \n",
    "        return file\n",
    "    else:\n",
    "        return path + '/' + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24dd206d-9074-49a1-8373-3a0f9dfd892a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_audio_file(file, defFs, checkMono):\n",
    "    signal, Fs = librosa.load(file, sr=None, mono=False)\n",
    "    if defFs != Fs:\n",
    "        print('sampling rate mismatch, %d != %d for file %s'%(defFs, Fs, file))\n",
    "        return False\n",
    "    elif checkMono == True:\n",
    "        if signal.ndim != 1:\n",
    "            print('not mono file %s, shape='%(file), signal.shape)\n",
    "            return False\n",
    "        return True\n",
    "    elif size(signal) <= 0:\n",
    "        print('wrong audio file %s, shape='%(file), signal.shape)\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def convert_audio_file(file, forceFs, forceMono):\n",
    "    signal, Fs = librosa.load(file, sr=None, mono=False)\n",
    "    changed = False\n",
    "    if forceFs != Fs:\n",
    "        print('sampling rate mismatch, %d != %d for file %s'%(forceFs, Fs, file))\n",
    "        signal, Fs = librosa.load(file, sr=forceFs, mono=False)\n",
    "        changed = True\n",
    "    elif forceMono == True:\n",
    "        if signal.ndim != 1:\n",
    "            print('not mono file %s, shape='%(file), signal.shape)\n",
    "            signal, Fs = librosa.load(file, sr=forceFs, mono=True)\n",
    "            changed = True\n",
    "    elif size(signal) <= 0:\n",
    "        print('wrong audio file %s, shape='%(file), signal.shape)\n",
    "        return False\n",
    "    if changed == True:\n",
    "        wav.writewav(file, Fs, signal, maxval=1.0)\n",
    "        print('updating', file)\n",
    "    return changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f7c5bf3-1193-4279-b2fe-5815ac37b37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouYeNa: false 0 / 100\n",
      "\n",
      "deokkyukwon: false 0 / 100\n",
      "\n",
      "11jeonghy: false 0 / 100\n",
      "\n",
      "InkooJeon: false 0 / 100\n",
      "\n",
      "Dandyst: false 0 / 100\n",
      "\n",
      "ohjihyeon: false 0 / 100\n",
      "\n",
      "son: false 0 / 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainroot = 'segmented-train'\n",
    "'''\n",
    "labels_all = {'11jeonghy', \n",
    "                'Dandyst', \n",
    "                'InkooJeon',\n",
    "                'YouYeNa',\n",
    "                'chlee',\n",
    "                'deokkyukwon',\n",
    "                'do',\n",
    "                'kyeong',\n",
    "                'ohjihyeon',\n",
    "                'son',\n",
    "               }\n",
    "'''\n",
    "labels_train = {'11jeonghy', \n",
    "                'Dandyst', \n",
    "                'InkooJeon',\n",
    "                'YouYeNa',\n",
    "                'deokkyukwon',\n",
    "                'ohjihyeon',\n",
    "                'son',\n",
    "               }\n",
    "\n",
    "# check\n",
    "Fs = 16000\n",
    "for subname in labels_train:\n",
    "    num_files = 0\n",
    "    num_false_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            file = addpath(trainroot, addpath(subname, basename))\n",
    "            num_files += 1\n",
    "            if check_audio_file(file, Fs, True) == False:\n",
    "                num_false_files += 1\n",
    "    print('%s: false %d / %d\\n'%(subname, num_false_files, num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "342c0678-3f89-45cb-af59-b6f088ef6058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do: false 0 / 100\n",
      "\n",
      "kyeong: false 0 / 100\n",
      "\n",
      "chlee: false 0 / 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valroot = 'segmented-val'\n",
    "valclean = addpath(valroot, 'clean')\n",
    "labels_val = {\n",
    "                'chlee',\n",
    "                'do',\n",
    "                'kyeong',\n",
    "               }\n",
    "\n",
    "# check\n",
    "Fs = 16000\n",
    "for subname in labels_val:\n",
    "    num_files = 0\n",
    "    num_false_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            file = addpath(valclean, addpath(subname, basename))\n",
    "            num_files += 1\n",
    "            if check_audio_file(file, Fs, True) == False:\n",
    "                num_false_files += 1\n",
    "    print('%s: false %d / %d\\n'%(subname, num_false_files, num_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073a311-4b6c-44c0-aa1c-4d9d666ad3ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### HMM training and test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24b52602-9088-4556-9b0e-259ede899922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartProbPrior=\n",
      "[1. 0. 0.]\n",
      "TransMatPrior=\n",
      "[[0.5 0.5 0. ]\n",
      " [0.  0.5 0.5]\n",
      " [0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scikits.talkbox.features import mfcc\n",
    "#librosa.feature.mfcc(*, y=None, sr=22050, S=None, n_mfcc=20, dct_type=2, norm='ortho', lifter=0, **kwargs)[source]\n",
    "from librosa.feature import mfcc\n",
    "from scipy.io import wavfile\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import scipy.stats as sp\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "############################################################################################## \n",
    "# extract MFCC features\n",
    "def extmfcc(file):\n",
    "    samplerate, d = wavfile.read(file)\n",
    "    #features.append(mfcc(d, nwin=int(samplerate * 0.03), fs=samplerate, nceps= 6)[0])\n",
    "    x = np.float32(d)\n",
    "    hop=samplerate//100\n",
    "    mc = mfcc(y=x, sr=samplerate, n_mfcc=num_mfcc, hop_length=hop, win_length=hop*2)\n",
    "    return np.transpose(mc, (1,0))\n",
    "\n",
    "def initByBakis(inumstates, ibakisLevel):\n",
    "    startprobPrior = np.zeros(inumstates)\n",
    "    startprobPrior[0: ibakisLevel - 1] = 1/float((ibakisLevel - 1))\n",
    "    transmatPrior = getTransmatPrior(inumstates, ibakisLevel)\n",
    "    return startprobPrior, transmatPrior\n",
    "\n",
    "def getTransmatPrior(inumstates, ibakisLevel):\n",
    "    transmatPrior = (1 / float(ibakisLevel)) * np.eye(inumstates)\n",
    "\n",
    "    for i in range(inumstates - (ibakisLevel - 1)):\n",
    "        for j in range(ibakisLevel - 1):\n",
    "            transmatPrior[i, i + j + 1] = 1. / ibakisLevel\n",
    "\n",
    "    for i in range(inumstates - ibakisLevel + 1, inumstates):\n",
    "        for j in range(inumstates - i - j):\n",
    "            transmatPrior[i, i + j] = 1. / (inumstates - i)\n",
    "\n",
    "    return transmatPrior\n",
    "\n",
    "\n",
    "############################################################################################## \n",
    "# hyperparameters - CHANGE THEM TO IMPROVE PERFORMANCE\n",
    "# 1. number of MFCC (feature dimension)\n",
    "num_mfcc = 6\n",
    "#num_mfcc = 10\n",
    "#num_mfcc = 13\n",
    "# 2. Parameters needed to train GMMHMM\n",
    "m_num_of_HMMStates = 3  # number of states\n",
    "m_num_of_mixtures = 2  # number of mixtures for each hidden state\n",
    "m_covarianceType = 'diag'  # covariance type\n",
    "m_n_iter = 10  # number of iterations\n",
    "m_bakisLevel = 2\n",
    "m_startprobPrior, m_transmatPrior = initByBakis(m_num_of_HMMStates,m_bakisLevel)\n",
    "print(\"StartProbPrior=\"); print(m_startprobPrior)\n",
    "print(\"TransMatPrior=\"); print(m_transmatPrior)\n",
    "\n",
    "\n",
    "############################################################################################## \n",
    "# acoustic model definition\n",
    "class SpeechModel:\n",
    "    def __init__(self,Class,label):\n",
    "        self.traindata = np.zeros((0,num_mfcc))\n",
    "        self.Class = Class\n",
    "        self.label = label\n",
    "        self.model  = hmm.GMMHMM(n_components = m_num_of_HMMStates, n_mix = m_num_of_mixtures, \\\n",
    "                transmat_prior = m_transmatPrior, startprob_prior = m_startprobPrior, \\\n",
    "                covariance_type = m_covarianceType, n_iter = m_n_iter)\n",
    "\n",
    "##################################################################################\n",
    "# folder structure:\n",
    "#  ${rootpath} / ${speaker_name} / m:0-9 / ${tag}[t:0-${numtrials}]-[m:0-9]\n",
    "#    m:0-9 model number\n",
    "#    t:0-{numtrials} trial number\n",
    "#  example: train_digits('segmented-train', {'gjang', 'do', 'son'}, 'kdigis', 10) \n",
    "#           will train with\n",
    "#    segmented-train/gjang/0/kdigits0-0.wav\n",
    "#    segmented-train/gjang/0/kdigits1-0.wav\n",
    "#    ...\n",
    "#    segmented-train/son/9/kdigits8-9.wav\n",
    "#    segmented-train/son/9/kdigits9-9.wav\n",
    "##################################################################################\n",
    "def train_digits(rootpath, speakers, tag, num_trials=10):    \n",
    "    ############################################################################################## \n",
    "    # 1. find files\n",
    "    #    for user \"gjang\", digit 2, recording trial 0 (1st)\n",
    "    #    \"segmented/gjang/2/kdigits0-2.wav\"\n",
    "    # 2. extract MFCC features for training and testing\n",
    "    #    for each digit, indexes 4 and 9 for test, and the rest for training\n",
    "\n",
    "    #fpaths = []\n",
    "    #labels = []\n",
    "    spoken = []\n",
    "    m_trainingsetfeatures = []\n",
    "    m_trainingsetlabels = []\n",
    "\n",
    "    count = 0\n",
    "    for username in speakers:\n",
    "        apath2 = addpath(rootpath, username)    # example: segmented/gjang\n",
    "        for ii in range(10):   #dnum in os.listdir(apath2):\n",
    "            dnum = str(ii)\n",
    "            apath3 = addpath(apath2, dnum)     # example: segmented/gjang/2\n",
    "            if dnum not in spoken:\n",
    "                spoken.append(dnum)\n",
    "            for trial in range(num_trials):\n",
    "                file = addpath(apath3,\"{}{}-{}.wav\".format(tag,trial,dnum))      # segmented/gjang/2/kdigits0-2.wav\n",
    "                mc = extmfcc(file)\n",
    "\n",
    "                # display file names for the first 20 files only\n",
    "                count += 1\n",
    "                if count <= 20:\n",
    "                    print(file, dnum, end=' '); print(mc.shape, end=' ')\n",
    "                elif count == 21:\n",
    "                    print('...'); print('')\n",
    "\n",
    "                m_trainingsetfeatures.append(mc)\n",
    "                m_trainingsetlabels.append(dnum)\n",
    "\n",
    "    print('Words spoken:', spoken)\n",
    "    #print(\"number of labels and features = %d, %d\" % ( len(labels), len(features) ))\n",
    "    #print(\"feature shape = \", end='')\n",
    "    #print(features[0].shape)\n",
    "\n",
    "    ############################################################################################## \n",
    "    ntrain = len(m_trainingsetlabels)\n",
    "\n",
    "    print(\"[training] number of labels and features = %d, %d\" % \n",
    "            ( len(m_trainingsetlabels), len(m_trainingsetfeatures)) )\n",
    "    print ('Loading data completed')\n",
    "\n",
    "    ############################################################################################## \n",
    "    # model initialization\n",
    "    gmmhmmindexdict = {}\n",
    "    index = 0\n",
    "    for word in spoken:\n",
    "        gmmhmmindexdict[word] = index\n",
    "        index = index +1\n",
    "\n",
    "    ############################################################################################## \n",
    "    # training GMMHMM Models \n",
    "    start = time()\n",
    "\n",
    "    speechmodels = [None] * len(spoken)\n",
    "    for key in gmmhmmindexdict:\n",
    "        speechmodels[gmmhmmindexdict[key]] = SpeechModel(gmmhmmindexdict[key],key)\n",
    "\n",
    "    for i in range(0,len(m_trainingsetfeatures)):\n",
    "         for j in range(0,len(speechmodels)):\n",
    "             if int(speechmodels[j].Class) == int(gmmhmmindexdict[m_trainingsetlabels[i]]):\n",
    "                speechmodels[j].traindata = np.concatenate((speechmodels[j].traindata , m_trainingsetfeatures[i]))\n",
    "\n",
    "    for speechmodel in speechmodels:\n",
    "        speechmodel.model.fit(speechmodel.traindata)\n",
    "\n",
    "    print ('Training completed -- {0} GMM-HMM models are built for {0} different types of words'.format(len(spoken)))\n",
    "    print('time elapsed: %.2f seconds' % ( time() - start ))\n",
    "    print (\" \"); print(\" \")\n",
    "    \n",
    "    return speechmodels, gmmhmmindexdict\n",
    "\n",
    "    '''\n",
    "    ############################################################################################## \n",
    "    # testing\n",
    "    print(\"Prediction with training data started\")\n",
    "    m_PredictionlabelList = []\n",
    "\n",
    "    for i in range(0,len(m_testingsetfeatures)):\n",
    "        scores = []\n",
    "        for speechmodel in speechmodels:\n",
    "             scores.append(speechmodel.model.score(m_testingsetfeatures[i]))\n",
    "        id  = scores.index(max(scores))\n",
    "        m_PredictionlabelList.append(speechmodels[id].Class)\n",
    "        print(str(np.round(scores, 3)) + \" \" + str(max(np.round(scores, 3))) +\" \"+\":\"+ speechmodels[id].label)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    count = 0\n",
    "    print(\"\")\n",
    "    print(\"Prediction for Testing DataSet:\")\n",
    "\n",
    "    for i in range(0,len(m_testingsetlabels)):\n",
    "        print( \"Label\"+str(i+1)+\":\"+m_testingsetlabels[i])\n",
    "        if gmmhmmindexdict[m_testingsetlabels[i]] == m_PredictionlabelList[i]:\n",
    "           count = count+1\n",
    "\n",
    "    accuracy = 100.0*count/float(len(m_testingsetlabels))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"accuracy =\"+str(accuracy))\n",
    "    print(\"\")\n",
    "\n",
    "    ############################################################################################## \n",
    "    # end of testing\n",
    "    ############################################################################################## \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08525bf8-b5bc-4b28-87fa-f304681efa94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scikits.talkbox.features import mfcc\n",
    "#librosa.feature.mfcc(*, y=None, sr=22050, S=None, n_mfcc=20, dct_type=2, norm='ortho', lifter=0, **kwargs)[source]\n",
    "from librosa.feature import mfcc\n",
    "from scipy.io import wavfile\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import scipy.stats as sp\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##################################################################################\n",
    "# folder structure:\n",
    "#  ${rootpath} / ${speaker_name} / m:0-9 / ${tag}[t:0-${numtrials}]-[m:0-9]\n",
    "#    m:0-9 model number\n",
    "#    t:0-{numtrials} trial number\n",
    "#  example: train_digits('segmented-train', {'gjang', 'do', 'son'}, 'kdigis', 10) \n",
    "#           will train with\n",
    "#    segmented-train/gjang/0/kdigits0-0.wav\n",
    "#    segmented-train/gjang/0/kdigits1-0.wav\n",
    "#    ...\n",
    "#    segmented-train/son/9/kdigits8-9.wav\n",
    "#    segmented-train/son/9/kdigits9-9.wav\n",
    "##################################################################################\n",
    "def validation_digits(speechmodels, gmmhmmindexdict, rootpath, speakers, tag, num_trials=10):    \n",
    "\n",
    "    ############################################################################################## \n",
    "    # 1. find files\n",
    "    #    for user \"gjang\", digit 2, recording trial 0 (1st)\n",
    "    #    \"segmented/gjang/2/kdigits0-2.wav\"\n",
    "    # 2. extract MFCC features for training and testing\n",
    "    #    for each digit, indexes 4 and 9 for test, and the rest for training\n",
    "\n",
    "    #fpaths = []\n",
    "    #labels = []\n",
    "    spoken = []\n",
    "    m_features = []\n",
    "    m_labels = []\n",
    "\n",
    "    count = 0\n",
    "    for username in speakers:\n",
    "        apath2 = addpath(rootpath, username)    # example: segmented/gjang\n",
    "        for ii in range(10):   #dnum in os.listdir(apath2):\n",
    "            dnum = str(ii)\n",
    "            apath3 = addpath(apath2, dnum)     # example: segmented/gjang/2\n",
    "            if dnum not in spoken:\n",
    "                spoken.append(dnum)\n",
    "            for trial in range(num_trials):\n",
    "                file = addpath(apath3,\"{}{}-{}.wav\".format(tag,trial,dnum))      # segmented/gjang/2/kdigits0-2.wav\n",
    "                mc = extmfcc(file)\n",
    "\n",
    "                # display file names for the first 20 files only\n",
    "                count += 1\n",
    "                if count <= 20:\n",
    "                    print(file, dnum, end=' '); print(mc.shape, end=' ')\n",
    "                elif count == 21:\n",
    "                    print('...'); print('')\n",
    "\n",
    "                m_features.append(mc)\n",
    "                m_labels.append(dnum)\n",
    "\n",
    "    print('Words spoken:', spoken)\n",
    "    #print(\"number of labels and features = %d, %d\" % ( len(labels), len(features) ))\n",
    "    #print(\"feature shape = \", end='')\n",
    "    #print(features[0].shape)\n",
    "\n",
    "    ############################################################################################## \n",
    "    print(\"[validation] number of labels and features = %d, %d\" % ( len(m_labels), len(m_features)) )\n",
    "    print ('Loading data completed')\n",
    "\n",
    "    ############################################################################################## \n",
    "    # testing\n",
    "    print(\"Prediction started\")\n",
    "    m_PredictionlabelList = []\n",
    "\n",
    "    for i in range(0,len(m_features)):\n",
    "        scores = []\n",
    "        for speechmodel in speechmodels:\n",
    "             scores.append(speechmodel.model.score(m_features[i]))\n",
    "        id  = scores.index(max(scores))\n",
    "        m_PredictionlabelList.append(speechmodels[id].Class)\n",
    "        #print(str(np.round(scores, 3)) + \" \" + str(max(np.round(scores, 3))) +\" \"+\":\"+ speechmodels[id].label)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    count = 0\n",
    "    print(\"\")\n",
    "    print(\"Prediction for Testing DataSet:\")\n",
    "\n",
    "    for i in range(0,len(m_labels)):\n",
    "        #print( \"Label\"+str(i+1)+\":\"+m_labels[i])\n",
    "        if gmmhmmindexdict[m_labels[i]] == m_PredictionlabelList[i]:\n",
    "           count = count+1\n",
    "\n",
    "    accuracy = 100.0*count/float(len(m_labels))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"accuracy =\"+str(accuracy))\n",
    "    print(\"\")\n",
    "\n",
    "    ############################################################################################## \n",
    "    # end of testing\n",
    "    ############################################################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d39427-c914-462d-adf7-80a5b3882ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmented-train/YouYeNa/0/kdigits0-0.wav 0 (211, 6) segmented-train/YouYeNa/0/kdigits1-0.wav 0 (204, 6) segmented-train/YouYeNa/0/kdigits2-0.wav 0 (222, 6) segmented-train/YouYeNa/0/kdigits3-0.wav 0 (206, 6) segmented-train/YouYeNa/0/kdigits4-0.wav 0 (206, 6) segmented-train/YouYeNa/0/kdigits5-0.wav 0 (203, 6) segmented-train/YouYeNa/0/kdigits6-0.wav 0 (208, 6) segmented-train/YouYeNa/0/kdigits7-0.wav 0 (203, 6) segmented-train/YouYeNa/0/kdigits8-0.wav 0 (203, 6) segmented-train/YouYeNa/0/kdigits9-0.wav 0 (209, 6) segmented-train/YouYeNa/1/kdigits0-1.wav 1 (206, 6) segmented-train/YouYeNa/1/kdigits1-1.wav 1 (192, 6) segmented-train/YouYeNa/1/kdigits2-1.wav 1 (202, 6) segmented-train/YouYeNa/1/kdigits3-1.wav 1 (228, 6) segmented-train/YouYeNa/1/kdigits4-1.wav 1 (206, 6) segmented-train/YouYeNa/1/kdigits5-1.wav 1 (203, 6) segmented-train/YouYeNa/1/kdigits6-1.wav 1 (218, 6) segmented-train/YouYeNa/1/kdigits7-1.wav 1 (214, 6) segmented-train/YouYeNa/1/kdigits8-1.wav 1 (206, 6) segmented-train/YouYeNa/1/kdigits9-1.wav 1 (244, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[training] number of labels and features = 700, 700\n",
      "Loading data completed\n",
      "Training completed -- 10 GMM-HMM models are built for 10 different types of words\n",
      "time elapsed: 45.32 seconds\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "speechmodels, gmmhmmindexdict = train_digits(trainroot, labels_train, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afc22385-4942-4995-aef2-8e3a236d73ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmented-train/YouYeNa/0/kdigits0-0.wav 0 (211, 6) segmented-train/YouYeNa/0/kdigits1-0.wav 0 (204, 6) segmented-train/YouYeNa/0/kdigits2-0.wav 0 (222, 6) segmented-train/YouYeNa/0/kdigits3-0.wav 0 (206, 6) segmented-train/YouYeNa/0/kdigits4-0.wav 0 (206, 6) segmented-train/YouYeNa/0/kdigits5-0.wav 0 (203, 6) segmented-train/YouYeNa/0/kdigits6-0.wav 0 (208, 6) segmented-train/YouYeNa/0/kdigits7-0.wav 0 (203, 6) segmented-train/YouYeNa/0/kdigits8-0.wav 0 (203, 6) segmented-train/YouYeNa/0/kdigits9-0.wav 0 (209, 6) segmented-train/YouYeNa/1/kdigits0-1.wav 1 (206, 6) segmented-train/YouYeNa/1/kdigits1-1.wav 1 (192, 6) segmented-train/YouYeNa/1/kdigits2-1.wav 1 (202, 6) segmented-train/YouYeNa/1/kdigits3-1.wav 1 (228, 6) segmented-train/YouYeNa/1/kdigits4-1.wav 1 (206, 6) segmented-train/YouYeNa/1/kdigits5-1.wav 1 (203, 6) segmented-train/YouYeNa/1/kdigits6-1.wav 1 (218, 6) segmented-train/YouYeNa/1/kdigits7-1.wav 1 (214, 6) segmented-train/YouYeNa/1/kdigits8-1.wav 1 (206, 6) segmented-train/YouYeNa/1/kdigits9-1.wav 1 (244, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 700, 700\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =46.285714285714285\n",
      "\n",
      "segmented-val/clean/do/0/kdigits0-0.wav 0 (92, 6) segmented-val/clean/do/0/kdigits1-0.wav 0 (93, 6) segmented-val/clean/do/0/kdigits2-0.wav 0 (87, 6) segmented-val/clean/do/0/kdigits3-0.wav 0 (87, 6) segmented-val/clean/do/0/kdigits4-0.wav 0 (86, 6) segmented-val/clean/do/0/kdigits5-0.wav 0 (94, 6) segmented-val/clean/do/0/kdigits6-0.wav 0 (81, 6) segmented-val/clean/do/0/kdigits7-0.wav 0 (99, 6) segmented-val/clean/do/0/kdigits8-0.wav 0 (97, 6) segmented-val/clean/do/0/kdigits9-0.wav 0 (87, 6) segmented-val/clean/do/1/kdigits0-1.wav 1 (81, 6) segmented-val/clean/do/1/kdigits1-1.wav 1 (69, 6) segmented-val/clean/do/1/kdigits2-1.wav 1 (83, 6) segmented-val/clean/do/1/kdigits3-1.wav 1 (81, 6) segmented-val/clean/do/1/kdigits4-1.wav 1 (79, 6) segmented-val/clean/do/1/kdigits5-1.wav 1 (68, 6) segmented-val/clean/do/1/kdigits6-1.wav 1 (60, 6) segmented-val/clean/do/1/kdigits7-1.wav 1 (69, 6) segmented-val/clean/do/1/kdigits8-1.wav 1 (71, 6) segmented-val/clean/do/1/kdigits9-1.wav 1 (74, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =25.333333333333332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_digits(speechmodels, gmmhmmindexdict, trainroot, labels_train, 'kdigits', num_trials=10)\n",
    "validation_digits(speechmodels, gmmhmmindexdict, valclean, labels_val, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cdf79c-6d34-4c91-bcf3-993188dcb566",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### noise 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e77f9be2-c131-40fc-99e9-d42b7c645212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../audio/car.wav (175745,) [-0.01342773 -0.0222168  -0.02905273 ... -0.0390625  -0.03930664\n",
      " -0.04086304]\n",
      "../audio/car_wideband.wav (175745,) [-0.05984497 -0.14807129 -0.14700317 ... -0.10241699 -0.10253906\n",
      " -0.09594727]\n",
      "Fs = 16000, Ns = 160, Nf = 320, NFFT = 512, hNo = 257\n"
     ]
    }
   ],
   "source": [
    "audioinputpath = '../audio'\n",
    "noisefile  = addpath(audioinputpath, 'car.wav')\n",
    "wnoisefile  = addpath(audioinputpath, 'car_wideband.wav')   # 넓은 주파수 대역에 분포한 잡음\n",
    "\n",
    "Fs=16000\n",
    "noise, _ = librosa.load(noisefile, sr=Fs, mono=True)\n",
    "wnoise, _ = librosa.load(wnoisefile, sr=Fs, mono=True)\n",
    "# sr: target sampling rate. ‘None’ uses the native sampling rate\n",
    "# mono = True: convert signal to mono\n",
    "\n",
    "print(noisefile, noise.shape, noise)\n",
    "print(wnoisefile, wnoise.shape, wnoise)\n",
    "\n",
    "Ns = int(Fs*Ts)    # shift number of samples\n",
    "Nf = int(Fs*Tf)    # frame number of samples\n",
    "NFFT = int(2**(np.ceil(np.log2(Nf))))   # Nf보다 크거나 같은 2의 거듭제곱을 NFFT 로 정의\n",
    "hNo = NFFT//2+1\n",
    "print('Fs = %d, Ns = %d, Nf = %d, NFFT = %d, hNo = %d' % (Fs, Ns, Nf, NFFT, hNo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf48972-47cc-4af7-8de7-96a8dd40676f",
   "metadata": {},
   "source": [
    "__generate noisy speech with various SNRs__\n",
    "- 음성과 잡음의 상대적 크기에 따라 잡음의 효과를 time domain, spectrogram, 그리고 들어서 확인해 본다.\n",
    "- mixed input $x[t]$ 를 다음과 같이 생성한다.\n",
    "$$ x[t] = s[t] + 10^{-r/20} \\frac{\\sigma_{s}}{\\sigma_{n}} n[t] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "620a2571-2da9-4505-b211-2cfa13816693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_mixed_signals_2(speech, noise, SNRs, isdraw=False):\n",
    "    std_s = np.sqrt(np.mean(speech**2))\n",
    "    std_n = np.sqrt(np.mean(noise[:len(speech)]**2))\n",
    "    mixedSig = []\n",
    "    for snr in SNRs:\n",
    "        gain = np.power(10, -snr/20)\n",
    "        gn = noise[:len(speech)]/std_n*std_s*gain\n",
    "        m = speech + gn\n",
    "        mixedSig.append(m)\n",
    "\n",
    "    return mixedSig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "754fc25f-425b-4277-885b-411d5d65f067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audioroot = valroot\n",
    "audioclean = valclean\n",
    "labels = labels_val\n",
    "noisyroots = [addpath(audioroot,'nbnSNR'), addpath(audioroot,'wbnSNR')]\n",
    "SNRs = [10, 0, -10]\n",
    "\n",
    "for subname in labels:\n",
    "    num_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            infile = addpath(audioclean, addpath(subname, basename))            \n",
    "            num_files += 1\n",
    "            \n",
    "            signal, Fs = librosa.load(infile, sr=Fs, mono=True)\n",
    "            nbnsig = generate_mixed_signals_2(signal, noise, SNRs, False)\n",
    "            wbnsig = generate_mixed_signals_2(signal, wnoise, SNRs, False)\n",
    "            noisy = [nbnsig, wbnsig]\n",
    "            \n",
    "            for jj in range(len(noisy)):\n",
    "                for n in range(len(noisy[jj])):\n",
    "                    outfile = addpath('%s%d'%(noisyroots[jj],SNRs[n]), addpath(subname, basename))\n",
    "                    wav.writewav(outfile, Fs, noisy[jj][n], maxval=1.0)\n",
    "\n",
    "outputpaths = []\n",
    "for jj in range(len(noisy)):\n",
    "    for n in range(len(noisy[jj])):\n",
    "        outputpaths.append('%s%d'%(noisyroots[jj],SNRs[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54809aa7-63c6-4e11-b42e-8f59ef3495df",
   "metadata": {},
   "source": [
    "Noise model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46abea0e-484f-4dd2-ba19-d66a44178ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "testing segmented-val/nbnSNR10\n",
      "segmented-val/nbnSNR10/do/0/kdigits0-0.wav 0 (92, 6) segmented-val/nbnSNR10/do/0/kdigits1-0.wav 0 (93, 6) segmented-val/nbnSNR10/do/0/kdigits2-0.wav 0 (87, 6) segmented-val/nbnSNR10/do/0/kdigits3-0.wav 0 (87, 6) segmented-val/nbnSNR10/do/0/kdigits4-0.wav 0 (86, 6) segmented-val/nbnSNR10/do/0/kdigits5-0.wav 0 (94, 6) segmented-val/nbnSNR10/do/0/kdigits6-0.wav 0 (81, 6) segmented-val/nbnSNR10/do/0/kdigits7-0.wav 0 (99, 6) segmented-val/nbnSNR10/do/0/kdigits8-0.wav 0 (97, 6) segmented-val/nbnSNR10/do/0/kdigits9-0.wav 0 (87, 6) segmented-val/nbnSNR10/do/1/kdigits0-1.wav 1 (81, 6) segmented-val/nbnSNR10/do/1/kdigits1-1.wav 1 (69, 6) segmented-val/nbnSNR10/do/1/kdigits2-1.wav 1 (83, 6) segmented-val/nbnSNR10/do/1/kdigits3-1.wav 1 (81, 6) segmented-val/nbnSNR10/do/1/kdigits4-1.wav 1 (79, 6) segmented-val/nbnSNR10/do/1/kdigits5-1.wav 1 (68, 6) segmented-val/nbnSNR10/do/1/kdigits6-1.wav 1 (60, 6) segmented-val/nbnSNR10/do/1/kdigits7-1.wav 1 (69, 6) segmented-val/nbnSNR10/do/1/kdigits8-1.wav 1 (71, 6) segmented-val/nbnSNR10/do/1/kdigits9-1.wav 1 (74, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =29.666666666666668\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/nbnSNR0\n",
      "segmented-val/nbnSNR0/do/0/kdigits0-0.wav 0 (92, 6) segmented-val/nbnSNR0/do/0/kdigits1-0.wav 0 (93, 6) segmented-val/nbnSNR0/do/0/kdigits2-0.wav 0 (87, 6) segmented-val/nbnSNR0/do/0/kdigits3-0.wav 0 (87, 6) segmented-val/nbnSNR0/do/0/kdigits4-0.wav 0 (86, 6) segmented-val/nbnSNR0/do/0/kdigits5-0.wav 0 (94, 6) segmented-val/nbnSNR0/do/0/kdigits6-0.wav 0 (81, 6) segmented-val/nbnSNR0/do/0/kdigits7-0.wav 0 (99, 6) segmented-val/nbnSNR0/do/0/kdigits8-0.wav 0 (97, 6) segmented-val/nbnSNR0/do/0/kdigits9-0.wav 0 (87, 6) segmented-val/nbnSNR0/do/1/kdigits0-1.wav 1 (81, 6) segmented-val/nbnSNR0/do/1/kdigits1-1.wav 1 (69, 6) segmented-val/nbnSNR0/do/1/kdigits2-1.wav 1 (83, 6) segmented-val/nbnSNR0/do/1/kdigits3-1.wav 1 (81, 6) segmented-val/nbnSNR0/do/1/kdigits4-1.wav 1 (79, 6) segmented-val/nbnSNR0/do/1/kdigits5-1.wav 1 (68, 6) segmented-val/nbnSNR0/do/1/kdigits6-1.wav 1 (60, 6) segmented-val/nbnSNR0/do/1/kdigits7-1.wav 1 (69, 6) segmented-val/nbnSNR0/do/1/kdigits8-1.wav 1 (71, 6) segmented-val/nbnSNR0/do/1/kdigits9-1.wav 1 (74, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =25.0\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/nbnSNR-10\n",
      "segmented-val/nbnSNR-10/do/0/kdigits0-0.wav 0 (92, 6) segmented-val/nbnSNR-10/do/0/kdigits1-0.wav 0 (93, 6) segmented-val/nbnSNR-10/do/0/kdigits2-0.wav 0 (87, 6) segmented-val/nbnSNR-10/do/0/kdigits3-0.wav 0 (87, 6) segmented-val/nbnSNR-10/do/0/kdigits4-0.wav 0 (86, 6) segmented-val/nbnSNR-10/do/0/kdigits5-0.wav 0 (94, 6) segmented-val/nbnSNR-10/do/0/kdigits6-0.wav 0 (81, 6) segmented-val/nbnSNR-10/do/0/kdigits7-0.wav 0 (99, 6) segmented-val/nbnSNR-10/do/0/kdigits8-0.wav 0 (97, 6) segmented-val/nbnSNR-10/do/0/kdigits9-0.wav 0 (87, 6) segmented-val/nbnSNR-10/do/1/kdigits0-1.wav 1 (81, 6) segmented-val/nbnSNR-10/do/1/kdigits1-1.wav 1 (69, 6) segmented-val/nbnSNR-10/do/1/kdigits2-1.wav 1 (83, 6) segmented-val/nbnSNR-10/do/1/kdigits3-1.wav 1 (81, 6) segmented-val/nbnSNR-10/do/1/kdigits4-1.wav 1 (79, 6) segmented-val/nbnSNR-10/do/1/kdigits5-1.wav 1 (68, 6) segmented-val/nbnSNR-10/do/1/kdigits6-1.wav 1 (60, 6) segmented-val/nbnSNR-10/do/1/kdigits7-1.wav 1 (69, 6) segmented-val/nbnSNR-10/do/1/kdigits8-1.wav 1 (71, 6) segmented-val/nbnSNR-10/do/1/kdigits9-1.wav 1 (74, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =11.333333333333334\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/wbnSNR10\n",
      "segmented-val/wbnSNR10/do/0/kdigits0-0.wav 0 (92, 6) segmented-val/wbnSNR10/do/0/kdigits1-0.wav 0 (93, 6) segmented-val/wbnSNR10/do/0/kdigits2-0.wav 0 (87, 6) segmented-val/wbnSNR10/do/0/kdigits3-0.wav 0 (87, 6) segmented-val/wbnSNR10/do/0/kdigits4-0.wav 0 (86, 6) segmented-val/wbnSNR10/do/0/kdigits5-0.wav 0 (94, 6) segmented-val/wbnSNR10/do/0/kdigits6-0.wav 0 (81, 6) segmented-val/wbnSNR10/do/0/kdigits7-0.wav 0 (99, 6) segmented-val/wbnSNR10/do/0/kdigits8-0.wav 0 (97, 6) segmented-val/wbnSNR10/do/0/kdigits9-0.wav 0 (87, 6) segmented-val/wbnSNR10/do/1/kdigits0-1.wav 1 (81, 6) segmented-val/wbnSNR10/do/1/kdigits1-1.wav 1 (69, 6) segmented-val/wbnSNR10/do/1/kdigits2-1.wav 1 (83, 6) segmented-val/wbnSNR10/do/1/kdigits3-1.wav 1 (81, 6) segmented-val/wbnSNR10/do/1/kdigits4-1.wav 1 (79, 6) segmented-val/wbnSNR10/do/1/kdigits5-1.wav 1 (68, 6) segmented-val/wbnSNR10/do/1/kdigits6-1.wav 1 (60, 6) segmented-val/wbnSNR10/do/1/kdigits7-1.wav 1 (69, 6) segmented-val/wbnSNR10/do/1/kdigits8-1.wav 1 (71, 6) segmented-val/wbnSNR10/do/1/kdigits9-1.wav 1 (74, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =20.333333333333332\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/wbnSNR0\n",
      "segmented-val/wbnSNR0/do/0/kdigits0-0.wav 0 (92, 6) segmented-val/wbnSNR0/do/0/kdigits1-0.wav 0 (93, 6) segmented-val/wbnSNR0/do/0/kdigits2-0.wav 0 (87, 6) segmented-val/wbnSNR0/do/0/kdigits3-0.wav 0 (87, 6) segmented-val/wbnSNR0/do/0/kdigits4-0.wav 0 (86, 6) segmented-val/wbnSNR0/do/0/kdigits5-0.wav 0 (94, 6) segmented-val/wbnSNR0/do/0/kdigits6-0.wav 0 (81, 6) segmented-val/wbnSNR0/do/0/kdigits7-0.wav 0 (99, 6) segmented-val/wbnSNR0/do/0/kdigits8-0.wav 0 (97, 6) segmented-val/wbnSNR0/do/0/kdigits9-0.wav 0 (87, 6) segmented-val/wbnSNR0/do/1/kdigits0-1.wav 1 (81, 6) segmented-val/wbnSNR0/do/1/kdigits1-1.wav 1 (69, 6) segmented-val/wbnSNR0/do/1/kdigits2-1.wav 1 (83, 6) segmented-val/wbnSNR0/do/1/kdigits3-1.wav 1 (81, 6) segmented-val/wbnSNR0/do/1/kdigits4-1.wav 1 (79, 6) segmented-val/wbnSNR0/do/1/kdigits5-1.wav 1 (68, 6) segmented-val/wbnSNR0/do/1/kdigits6-1.wav 1 (60, 6) segmented-val/wbnSNR0/do/1/kdigits7-1.wav 1 (69, 6) segmented-val/wbnSNR0/do/1/kdigits8-1.wav 1 (71, 6) segmented-val/wbnSNR0/do/1/kdigits9-1.wav 1 (74, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =18.333333333333332\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/wbnSNR-10\n",
      "segmented-val/wbnSNR-10/do/0/kdigits0-0.wav 0 (92, 6) segmented-val/wbnSNR-10/do/0/kdigits1-0.wav 0 (93, 6) segmented-val/wbnSNR-10/do/0/kdigits2-0.wav 0 (87, 6) segmented-val/wbnSNR-10/do/0/kdigits3-0.wav 0 (87, 6) segmented-val/wbnSNR-10/do/0/kdigits4-0.wav 0 (86, 6) segmented-val/wbnSNR-10/do/0/kdigits5-0.wav 0 (94, 6) segmented-val/wbnSNR-10/do/0/kdigits6-0.wav 0 (81, 6) segmented-val/wbnSNR-10/do/0/kdigits7-0.wav 0 (99, 6) segmented-val/wbnSNR-10/do/0/kdigits8-0.wav 0 (97, 6) segmented-val/wbnSNR-10/do/0/kdigits9-0.wav 0 (87, 6) segmented-val/wbnSNR-10/do/1/kdigits0-1.wav 1 (81, 6) segmented-val/wbnSNR-10/do/1/kdigits1-1.wav 1 (69, 6) segmented-val/wbnSNR-10/do/1/kdigits2-1.wav 1 (83, 6) segmented-val/wbnSNR-10/do/1/kdigits3-1.wav 1 (81, 6) segmented-val/wbnSNR-10/do/1/kdigits4-1.wav 1 (79, 6) segmented-val/wbnSNR-10/do/1/kdigits5-1.wav 1 (68, 6) segmented-val/wbnSNR-10/do/1/kdigits6-1.wav 1 (60, 6) segmented-val/wbnSNR-10/do/1/kdigits7-1.wav 1 (69, 6) segmented-val/wbnSNR-10/do/1/kdigits8-1.wav 1 (71, 6) segmented-val/wbnSNR-10/do/1/kdigits9-1.wav 1 (74, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.666666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in outputpaths:\n",
    "    print('--------------------------------')\n",
    "    print('testing', path)\n",
    "    validation_digits(speechmodels, gmmhmmindexdict, path, labels, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5330b-3c8a-4170-aedf-1376dceec3cf",
   "metadata": {},
   "source": [
    "unsegmented files (EPD needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f4d9cd6-9cd5-4cdc-95a2-72796fa3c95f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsegmented-test/clean/gjang/kdigits0.wav\n",
      "unsegmented-test/clean/gjang/kdigits1.wav\n",
      "unsegmented-test/clean/gjang/kdigits2.wav\n",
      "unsegmented-test/clean/gjang/kdigits3.wav\n",
      "unsegmented-test/clean/gjang/kdigits4.wav\n",
      "unsegmented-test/clean/gjang/kdigits5.wav\n",
      "unsegmented-test/clean/gjang/kdigits6.wav\n",
      "unsegmented-test/clean/gjang/kdigits7.wav\n",
      "unsegmented-test/clean/gjang/kdigits8.wav\n",
      "unsegmented-test/clean/gjang/kdigits9.wav\n"
     ]
    }
   ],
   "source": [
    "audioroot = 'unsegmented-test'\n",
    "audioclean = addpath(audioroot,'clean')\n",
    "labels = ['gjang']\n",
    "noisyroots = [addpath(audioroot,'nbnSNR'), addpath(audioroot,'wbnSNR')]\n",
    "SNRs = [10, 0, -10]\n",
    "\n",
    "for subname in labels:\n",
    "    num_files = 0\n",
    "    for trial in range(10):\n",
    "        basename = 'kdigits%d.wav'%(trial)\n",
    "        infile = addpath(audioclean, addpath(subname, basename))\n",
    "        print(infile)\n",
    "        num_files += 1\n",
    "\n",
    "        signal, Fs = librosa.load(infile, sr=Fs, mono=True)\n",
    "        nbnsig = generate_mixed_signals_2(signal, np.concatenate((noise,noise,noise)), SNRs, False)\n",
    "        wbnsig = generate_mixed_signals_2(signal, np.concatenate((wnoise,wnoise,wnoise)), SNRs, False)\n",
    "        noisy = [nbnsig, wbnsig]\n",
    "\n",
    "        for jj in range(len(noisy)):\n",
    "            for n in range(len(noisy[jj])):\n",
    "                outfile = addpath('%s%d'%(noisyroots[jj],SNRs[n]), addpath(subname, basename))\n",
    "                wav.writewav(outfile, Fs, noisy[jj][n], maxval=1.0)\n",
    "\n",
    "outputpaths = []\n",
    "for jj in range(len(noisy)):\n",
    "    for n in range(len(noisy[jj])):\n",
    "        outputpaths.append('%s%d'%(noisyroots[jj],SNRs[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615b35b-48f6-4617-9c55-0ee6a5f1ecdf",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8db264-367a-4400-9f13-86dc7347ce9a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2cc10f-ae74-4d39-bcdb-5863f58cb805",
   "metadata": {},
   "source": [
    "## End of Project 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
